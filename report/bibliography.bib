@misc{mnih2013playing,
  abstract = {We present the first deep learning model to successfully learn control
policies directly from high-dimensional sensory input using reinforcement
learning. The model is a convolutional neural network, trained with a variant
of Q-learning, whose input is raw pixels and whose output is a value function
estimating future rewards. We apply our method to seven Atari 2600 games from
the Arcade Learning Environment, with no adjustment of the architecture or
learning algorithm. We find that it outperforms all previous approaches on six
of the games and surpasses a human expert on three of them.},
  added-at = {2014-12-14T17:55:47.000+0100},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/2f4760edc252cd402821a341bda0026bf/vch},
  description = {Playing Atari with Deep Reinforcement Learning},
  interhash = {78966703f649bae69a08a6a23a4e8879},
  intrahash = {f4760edc252cd402821a341bda0026bf},
  keywords = {arxiv cs},
  note = {cite arxiv:1312.5602Comment: NIPS Deep Learning Workshop 2013},
  timestamp = {2014-12-14T17:55:47.000+0100},
  title = {Playing Atari with Deep Reinforcement Learning},
  url = {http://arxiv.org/abs/1312.5602},
  year = 2013
}

@article{mnih2015humanlevel,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2015-08-26T14:46:40.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}

@misc{openai-gym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}

@misc{openai-gym-baselines,
  author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
  title = {OpenAI Baselines},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openai/baselines}},
}

@misc{Medium-Pong-30min,
  title = {{S}peeding up {DQN} on {P}y{T}orch: how to solve {P}ong in 30 minutes},
  howpublished = {\url{https://medium.com/mlreview/speeding-up-dqn-on-pytorch-solving-pong-in-30-minutes-81a1bd2dff55}}
}

@article{Adam-Kingma,
  added-at = {2015-01-01T00:00:00.000+0100},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  biburl = {https://www.bibsonomy.org/bibtex/23b0328784dbfce338ba0dd2618a7a059/dblp},
  ee = {http://arxiv.org/abs/1412.6980},
  interhash = {57d2ac873f398f21bb94790081e80394},
  intrahash = {3b0328784dbfce338ba0dd2618a7a059},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2015-06-18T04:22:29.000+0200},
  title = {Adam: A Method for Stochastic Optimization.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1412.html#KingmaB14},
  volume = {abs/1412.6980},
  year = 2014
}

@inproceedings{mnih2016asynchronous,
  added-at = {2017-11-15T22:01:54.000+0100},
  author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  biburl = {https://www.bibsonomy.org/bibtex/265d8e8be4a5d4e16b732e749cf06593a/lukasw},
  booktitle = {International Conference on Machine Learning},
  interhash = {02e623113f85237b4ec7daf03736c6cc},
  intrahash = {65d8e8be4a5d4e16b732e749cf06593a},
  keywords = {atari final},
  pages = {1928--1937},
  timestamp = {2017-11-15T22:04:26.000+0100},
  title = {Asynchronous methods for deep reinforcement learning},
  url = {https://arxiv.org/pdf/1602.01783.pdf},
  year = 2016
}

@article{DBLP:journals/corr/SchaulQAS15,
  author    = {Tom Schaul and
               John Quan and
               Ioannis Antonoglou and
               David Silver},
  title     = {Prioritized Experience Replay},
  journal   = {CoRR},
  volume    = {abs/1511.05952},
  year      = {2015}
}

@article{babaeizadeh2016ga3c,
  title={{GA3C:} {GPU}-based {A3C} for Deep Reinforcement Learning},
  author={Babaeizadeh, Mohammad and Frosio, Iuri and Tyree, Stephen and Clemons, Jason and Kautz, Jan},
  journal={NIPS Workshop},
  biurl={arXiv preprint arXiv:1611.06256},
  year={2016}
}

@article{DBLP:journals/corr/abs-1802-01561,
  author    = {Lasse Espeholt and
               Hubert Soyer and
               R{\'{e}}mi Munos and
               Karen Simonyan and
               Volodymyr Mnih and
               Tom Ward and
               Yotam Doron and
               Vlad Firoiu and
               Tim Harley and
               Iain Dunning and
               Shane Legg and
               Koray Kavukcuoglu},
  title     = {{IMPALA:} Scalable Distributed Deep-RL with Importance Weighted Actor-Learner
               Architectures},
  journal   = {CoRR},
  volume    = {abs/1802.01561},
  year      = {2018}
}
